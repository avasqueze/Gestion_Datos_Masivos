# Curso de Gesti√≥n de Datos Masivos



> **Duraci√≥n:** 16 semanas ¬∑ 3 h/semana (36 h)  
> **Requisitos:** Python b√°sico, SQL b√°sico, Linux/CLI b√°sico.  
> **Tecnolog√≠as:** Python 3.10+, PySpark 3.x, Hadoop 3.x, Kafka (KRaft), Docker y Docker compose.

## Estructura de carpetas
```
Curso_BigData_12s/
‚îú‚îÄ‚îÄ docker/                # Docker Compose para Hadoop y Kafka
‚îú‚îÄ‚îÄ env/                   # Requisitos (pip/conda)
‚îú‚îÄ‚îÄ slides/                # 12 archivos .md (diapositivas por semana)
‚îú‚îÄ‚îÄ labs/
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/         # Notebooks de PySpark (ipynb)
‚îÇ   ‚îî‚îÄ‚îÄ guias/             # Instrucciones paso a paso (md)
‚îú‚îÄ‚îÄ scripts/               # Productor Kafka, utilidades y Makefile opcional
‚îú‚îÄ‚îÄ data/                  # Datos de ejemplo (CSV/JSONL) + generadores
‚îî‚îÄ‚îÄ evaluacion/            # R√∫bricas y quizzes
```

Aqu√≠ tienes un programa de **16 semanas (3 h/semana)** estructurado con los temas que indicaste, usando **Python, Spark y Hadoop** como ejes pr√°cticos. Tambi√©n incluyo un libro principal reciente (post-2020) y tres auxiliares que complementan el aprendizaje.

---

## üìö Bibliograf√≠a recomendada

**Libro principal**

* **‚ÄúData Analytics with Hadoop: An Introduction for Data Scientists‚Äù (2nd Ed., 2021)** ‚Äì Benjamin Bengfort, Jenny Kim, Michelle Brush.

  > Muy actualizado en ecosistema Hadoop, Spark, MapReduce y arquitecturas modernas para datos masivos, con ejemplos en Python.

**Libros auxiliares**

1. **‚ÄúLearning Spark: Lightning-Fast Data Analytics‚Äù (2nd Ed., 2020)** ‚Äì Jules Damji, Brooke Wenig, Tathagata Das, Denny Lee.

   > Gu√≠a pr√°ctica para Spark 3.x con Python, cubre batch y streaming.
2. **‚ÄúFundamentals of Data Engineering‚Äù (2022)** ‚Äì Joe Reis, Matt Housley.

   > Enfoque en arquitectura, ciclo de vida y pipelines de datos a gran escala.
3. **‚ÄúStreaming Systems: The What, Where, When, and How of Large-Scale Data Processing‚Äù (Updated Ed., 2021)** ‚Äì Tyler Akidau, Slava Chernyak, Reuven Lax.

   > Especializado en procesamiento streaming, arquitecturas Lambda/Kappa.

---

## üìÖ Programa del curso ‚Äì 16 semanas (3 h/semana)

### **Parte 1 ‚Äì Fundamentos y ciclo de vida (Semanas 1‚Äì5)**

| Semana | Tema                                                                      | Actividades/Pr√°cticas                                            |
| ------ | ------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| 1      | Introducci√≥n a Big Data, caracter√≠sticas (3V/5V), casos de uso            | Presentaci√≥n del curso, instalaci√≥n de Python y entorno Jupyter. |
| 2      | Datos distribuidos y escalabilidad                                        | Ejercicios con datos particionados en Python.                    |
| 3      | Costos computacionales de almacenamiento y consulta                       | Medici√≥n de tiempos y recursos en consultas.                     |
| 4      | Ciclo de vida de datos (adquisici√≥n, almacenamiento, an√°lisis, archivado) | Mini-proyecto: documentar un caso real de ciclo de vida.         |
| 5      | Herramientas y entornos de Big Data                                       | Instalaci√≥n local de Hadoop y Spark (modo standalone).           |

---

### **Parte 2 ‚Äì Ecosistemas y arquitecturas (Semanas 6‚Äì10)**

| Semana | Tema                                                                           | Actividades/Pr√°cticas                   |
| ------ | ------------------------------------------------------------------------------ | --------------------------------------- |
| 6      | Tipos de fuentes de datos: estructurados, no estructurados, semi-estructurados | Ejercicios con CSV, JSON, NoSQL.        |
| 7      | Ingesta y preparaci√≥n de datos masivos                                         | Uso de PySpark para ETL simple.         |
| 8      | Almacenamiento y flujos de trabajo                                             | Dise√±o de pipeline b√°sico con Spark.    |
| 9      | Arquitecturas de referencia (simple, Lambda, Kappa)                            | Discusi√≥n de casos reales y diagramado. |
| 10     | Ecosistema Hadoop: HDFS, YARN, Hive, Pig                                       | Lectura/escritura en HDFS con Python.   |

---

### **Parte 3 ‚Äì Procesamiento y arquitecturas avanzadas (Semanas 11‚Äì16)**

| Semana | Tema                                      | Actividades/Pr√°cticas                                    |
| ------ | ----------------------------------------- | -------------------------------------------------------- |
| 11     | MapReduce: fundamentos y ejecuci√≥n        | Implementar WordCount en Hadoop MapReduce y en PySpark.  |
| 12     | Sistemas de datos distribuidos Hadoop     | Pruebas con HDFS y consultas en Hive.                    |
| 13     | Arquitectura Lambda                       | Pipeline batch+streaming con Spark Structured Streaming. |
| 14     | Arquitectura Kappa                        | Pipeline streaming-only con Kafka + Spark.               |
| 15     | Almacenamiento y flujos batch y streaming | Comparativa de rendimiento batch vs streaming.           |
| 16     | Presentaci√≥n final de proyectos           | Defensa y retroalimentaci√≥n.                             |

---

## üìä Evaluaci√≥n ‚Äì Proyecto de Aula

* **Entrega 1 (30%, Semana 6)**
  *Dise√±o del caso de estudio y plan de arquitectura de datos (fuentes, ingesta, almacenamiento, procesamiento).*
* **Entrega 2 (30%, Semana 11)**
  *Implementaci√≥n parcial: ingesta, almacenamiento y flujo batch con Spark o Hadoop.*
* **Entrega 3 (40%, Semana 16)**
  *Implementaci√≥n final con arquitectura Lambda o Kappa, incorporando batch y/o streaming, documentaci√≥n y defensa oral.*

---

Si quieres, puedo prepararte tambi√©n **un cronograma con lecturas espec√≠ficas por semana de los libros recomendados**, as√≠ cada tema del programa queda vinculado a cap√≠tulos concretos y ejercicios pr√°cticos. As√≠ tendr√≠as un plan de lectura guiada junto al plan de clases.

¬øQuieres que lo arme?
