{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4496d209",
   "metadata": {},
   "source": [
    "# Tipos de fuentes de datos con **PySpark**: estructuradas, semiestructuradas y no estructuradas\n",
    "\n",
    "**Fecha:** 2025-09-19  \n",
    "**Creado con:** ChatGPT (GPT-5 Pro)\n",
    "\n",
    "Este cuaderno reescribe los ejemplos usando **PySpark** para mostrar cómo trabajar con\n",
    "datos **estructurados**, **semiestructurados** y **no estructurados** a escala.\n",
    "Incluye consultas con DataFrames, aplanamiento de JSON/XML, y procesamiento básico\n",
    "de texto e imágenes apoyado en Spark. Las figuras se generan con **matplotlib**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d2367",
   "metadata": {},
   "source": [
    "## Requisitos\n",
    "\n",
    "- **Java 8/11** y **PySpark 3.x** instalados.\n",
    "- Para XML con el conector de Spark: paquete `com.databricks:spark-xml_2.12` (opcional, ver fallback).\n",
    "- Bibliotecas de apoyo: `matplotlib`, `numpy`, `pillow` (para el ejemplo de imágenes).\n",
    "\n",
    "```bash\n",
    "# Opción rápida (entorno local o Colab)\n",
    "pip install pyspark matplotlib numpy pillow\n",
    "# Para leer XML (opcional)\n",
    "# spark-submit --packages com.databricks:spark-xml_2.12:0.18.0 ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119324a",
   "metadata": {},
   "source": [
    "## Arranque de la sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"FuentesDeDatos_PySpark\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    # Descomenta para añadir el conector XML si usas spark-submit interactivo:\n",
    "    # .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.18.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3fce78",
   "metadata": {},
   "source": [
    "---\n",
    "## 1) Datos **estructurados** con PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24851f1",
   "metadata": {},
   "source": [
    "### Ejemplo: `clientes` y `ventas` (JOIN y agregaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07545d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F, types as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Definimos esquemas explícitos ---\n",
    "schema_clientes = T.StructType([\n",
    "    T.StructField(\"cliente_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"nombre\", T.StringType(), False),\n",
    "    T.StructField(\"ciudad\", T.StringType(), True),\n",
    "    T.StructField(\"segmento\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "schema_ventas = T.StructType([\n",
    "    T.StructField(\"venta_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"fecha\", T.StringType(), False),  # parsearemos a date\n",
    "    T.StructField(\"cliente_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"categoria\", T.StringType(), False),\n",
    "    T.StructField(\"monto\", T.DoubleType(), False),\n",
    "])\n",
    "\n",
    "clientes_rows = [\n",
    "    (101, \"Ana\", \"Bogotá\", \"Retail\"),\n",
    "    (102, \"Bruno\", \"Medellín\", \"Enterprise\"),\n",
    "    (103, \"Carla\", \"Cali\", \"Retail\"),\n",
    "    (104, \"Diego\", \"Barranquilla\", \"SMB\"),\n",
    "    (105, \"Elena\", \"Bogotá\", \"Enterprise\"),\n",
    "    (106, \"Fabio\", \"Medellín\", \"SMB\"),\n",
    "]\n",
    "\n",
    "ventas_rows = [\n",
    "    (1,\"2025-01-10\",101,\"Hardware\", 250.50),\n",
    "    (2,\"2025-01-12\",102,\"Software\", 899.99),\n",
    "    (3,\"2025-01-13\",103,\"Servicios\",120.00),\n",
    "    (4,\"2025-01-15\",104,\"Hardware\", 450.00),\n",
    "    (5,\"2025-01-18\",105,\"Servicios\", 89.50),\n",
    "    (6,\"2025-02-01\",101,\"Software\", 1300.00),\n",
    "    (7,\"2025-02-03\",106,\"Hardware\", 99.90),\n",
    "    (8,\"2025-02-07\",105,\"Servicios\", 500.00),\n",
    "    (9,\"2025-02-08\",102,\"Software\", 650.00),\n",
    "    (10,\"2025-02-15\",103,\"Servicios\", 220.00),\n",
    "    (11,\"2025-03-01\",101,\"Hardware\", 1200.00),\n",
    "    (12,\"2025-03-05\",102,\"Software\", 75.00),\n",
    "    (13,\"2025-03-07\",103,\"Servicios\", 180.00),\n",
    "    (14,\"2025-03-15\",104,\"Hardware\", 330.00),\n",
    "    (15,\"2025-03-20\",106,\"Software\", 420.00),\n",
    "]\n",
    "\n",
    "clientes = spark.createDataFrame(clientes_rows, schema_clientes)\n",
    "ventas = spark.createDataFrame(ventas_rows, schema_ventas).withColumn(\"fecha\", F.to_date(\"fecha\"))\n",
    "\n",
    "df = ventas.join(clientes, on=\"cliente_id\", how=\"left\")\n",
    "\n",
    "# Agregación por categoría\n",
    "resumen_cat = (df.groupBy(\"categoria\").agg(F.sum(\"monto\").alias(\"monto_total\"))\n",
    "                 .orderBy(F.desc(\"monto_total\")))\n",
    "\n",
    "resumen_pd = resumen_cat.toPandas()\n",
    "\n",
    "# --- Visualización (un gráfico, sin estilos ni colores definidos) ---\n",
    "plt.figure()\n",
    "plt.bar(resumen_pd[\"categoria\"], resumen_pd[\"monto_total\"])\n",
    "plt.title(\"Monto total por categoría (PySpark)\")\n",
    "plt.xlabel(\"Categoría\")\n",
    "plt.ylabel(\"Monto (unidades monetarias)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9dd799",
   "metadata": {},
   "source": [
    "### Validación ligera de esquema y reglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipos observados\n",
    "print(\"dtypes:\", df.dtypes)\n",
    "\n",
    "# Nulos por columna\n",
    "nulls = df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "nulls.show()\n",
    "\n",
    "# Dominios y reglas\n",
    "dominio_cat = [\"Hardware\",\"Software\",\"Servicios\"]\n",
    "out_of_domain = df.filter(~F.col(\"categoria\").isin(dominio_cat)).count()\n",
    "negativos = df.filter(F.col(\"monto\") < 0).count()\n",
    "\n",
    "print(\"Filas con categoría fuera de dominio:\", out_of_domain)\n",
    "print(\"Filas con montos negativos:\", negativos)\n",
    "\n",
    "assert out_of_domain == 0, \"Se encontraron categorías fuera de dominio\"\n",
    "assert negativos == 0, \"Se encontraron montos negativos\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34649478",
   "metadata": {},
   "source": [
    "---\n",
    "## 2) Datos **semiestructurados** con PySpark\n",
    "\n",
    "**Definición.** No siguen un esquema tabular rígido, pero tienen **estructura explícita** (pares clave-valor, etiquetas, anidamiento).  \n",
    "**Ejemplos.** JSON/XML de APIs, logs de eventos.  \n",
    "**Operaciones.** Aplanamiento con `select`, `explode`, `from_json`, validación mediante esquemas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b067db",
   "metadata": {},
   "source": [
    "### Ejemplo A: Normalizar **JSON** de eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256968a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos de ejemplo con anidamiento y campos opcionales\n",
    "logs = [\n",
    "    {\n",
    "        \"timestamp\": \"2025-03-01T10:00:00Z\",\n",
    "        \"service\": \"auth\",\n",
    "        \"severity\": \"INFO\",\n",
    "        \"user\": {\"id\": 101, \"role\": \"admin\"},\n",
    "        \"action\": \"login\",\n",
    "        \"payload\": {\"ip\": \"10.0.0.1\", \"agent\": {\"browser\": \"Chrome\", \"version\": \"123\"}},\n",
    "    },\n",
    "    {\n",
    "        \"timestamp\": \"2025-03-01T10:05:12Z\",\n",
    "        \"service\": \"billing\",\n",
    "        \"severity\": \"WARN\",\n",
    "        \"action\": \"charge_failed\",\n",
    "        \"payload\": {\"ip\": \"10.0.0.2\", \"error_code\": \"CARD_DECLINED\"},\n",
    "    },\n",
    "    {\n",
    "        \"timestamp\": \"2025-03-01T10:06:48Z\",\n",
    "        \"service\": \"auth\",\n",
    "        \"severity\": \"ERROR\",\n",
    "        \"user\": {\"id\": 102, \"role\": \"viewer\"},\n",
    "        \"action\": \"login\",\n",
    "        \"payload\": {\"ip\": \"10.0.0.3\", \"agent\": {\"browser\": \"Firefox\", \"version\": \"124\"}},\n",
    "    },\n",
    "    {\n",
    "        \"timestamp\": \"2025-03-01T10:07:02Z\",\n",
    "        \"service\": \"orders\",\n",
    "        \"severity\": \"INFO\",\n",
    "        \"user\": {\"id\": 103, \"role\": \"editor\"},\n",
    "        \"action\": \"create_order\",\n",
    "        \"payload\": {\"ip\": \"10.0.0.4\", \"items\": [{\"sku\": \"A1\",\"qty\": 2},{\"sku\": \"B9\",\"qty\": 1}]},\n",
    "    },\n",
    "]\n",
    "\n",
    "# Esquema explícito (recomendado para estabilidad)\n",
    "schema_logs = T.StructType([\n",
    "    T.StructField(\"timestamp\", T.StringType(), True),\n",
    "    T.StructField(\"service\", T.StringType(), True),\n",
    "    T.StructField(\"severity\", T.StringType(), True),\n",
    "    T.StructField(\"action\", T.StringType(), True),\n",
    "    T.StructField(\"user\", T.StructType([\n",
    "        T.StructField(\"id\", T.IntegerType(), True),\n",
    "        T.StructField(\"role\", T.StringType(), True),\n",
    "    ]), True),\n",
    "    T.StructField(\"payload\", T.StructType([\n",
    "        T.StructField(\"ip\", T.StringType(), True),\n",
    "        T.StructField(\"agent\", T.StructType([\n",
    "            T.StructField(\"browser\", T.StringType(), True),\n",
    "            T.StructField(\"version\", T.StringType(), True),\n",
    "        ]), True),\n",
    "        T.StructField(\"error_code\", T.StringType(), True),\n",
    "        T.StructField(\"items\", T.ArrayType(T.StructType([\n",
    "            T.StructField(\"sku\", T.StringType(), True),\n",
    "            T.StructField(\"qty\", T.IntegerType(), True),\n",
    "        ])), True),\n",
    "    ]), True),\n",
    "])\n",
    "\n",
    "df_logs = spark.createDataFrame(logs, schema_logs)\n",
    "\n",
    "# Aplanamiento: seleccionamos campos útiles\n",
    "flat = df_logs.select(\n",
    "    \"timestamp\",\"service\",\"severity\",\"action\",\n",
    "    F.col(\"user.id\").alias(\"user_id\"),\n",
    "    F.col(\"user.role\").alias(\"user_role\"),\n",
    "    F.col(\"payload.ip\").alias(\"ip\"),\n",
    "    F.col(\"payload.agent.browser\").alias(\"browser\"),\n",
    "    F.col(\"payload.agent.version\").alias(\"browser_version\"),\n",
    "    F.col(\"payload.error_code\").alias(\"error_code\"),\n",
    "    F.col(\"payload.items\").alias(\"items\")\n",
    ")\n",
    "\n",
    "# Explode de items (mantiene eventos sin items con explode_outer)\n",
    "items_flat = flat.withColumn(\"item\", F.explode_outer(\"items\"))                  .select(\"*\",\n",
    "                         F.col(\"item.sku\").alias(\"sku\"),\n",
    "                         F.col(\"item.qty\").alias(\"qty\"))                  .drop(\"item\",\"items\")\n",
    "\n",
    "items_flat.show(truncate=False)\n",
    "\n",
    "# Métrica simple: eventos por severidad\n",
    "sev = df_logs.groupBy(\"severity\").count().orderBy(F.desc(\"count\"))\n",
    "sev_pd = sev.toPandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.bar(sev_pd[\"severity\"], sev_pd[\"count\"])\n",
    "plt.title(\"Eventos por severidad (PySpark)\")\n",
    "plt.xlabel(\"Severidad\")\n",
    "plt.ylabel(\"Cuenta\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176d5e9",
   "metadata": {},
   "source": [
    "### Ejemplo B: Extraer campos desde **XML**\n",
    "\n",
    "Mostramos dos rutas:\n",
    "1) **Conector `spark-xml`** (si está disponible).  \n",
    "2) **Fallback**: parsear XML con Python y luego **crear un DataFrame de Spark**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos un XML de ejemplo en disco\n",
    "from pathlib import Path\n",
    "xml_path = Path(\"/mnt/data/books.xml\")\n",
    "xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "xml_str = '''<catalog>\n",
    "  <book id=\"b1\">\n",
    "    <title>Ingeniería de Datos</title>\n",
    "    <author>A. Rivera</author>\n",
    "    <year>2021</year>\n",
    "    <price currency=\"USD\">39.90</price>\n",
    "    <tags><tag>ETL</tag><tag>Modelado</tag></tags>\n",
    "  </book>\n",
    "  <book id=\"b2\">\n",
    "    <title>APIs con Python</title>\n",
    "    <author>L. Gómez</author>\n",
    "    <year>2023</year>\n",
    "    <price currency=\"EUR\">29.00</price>\n",
    "    <tags><tag>API</tag><tag>Web</tag></tags>\n",
    "  </book>\n",
    "  <book id=\"b3\">\n",
    "    <title>Bases NoSQL</title>\n",
    "    <author>M. Torres</author>\n",
    "    <year>2020</year>\n",
    "    <price currency=\"USD\">35.50</price>\n",
    "    <tags><tag>NoSQL</tag><tag>Escalabilidad</tag></tags>\n",
    "  </book>\n",
    "</catalog>'''\n",
    "xml_path.write_text(xml_str, encoding=\"utf-8\")\n",
    "print(\"Escrito:\", xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1668da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intento 1: usando spark-xml (si está instalado)\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "df_books = None\n",
    "try:\n",
    "    df_xml = (spark.read.format(\"xml\")\n",
    "              .option(\"rowTag\", \"book\")\n",
    "              .load(str(xml_path)))\n",
    "    # Campos: _attr_id para atributo id, price._value, price._attr_currency, etc. según spark-xml\n",
    "    # Normalizamos columnas posibles\n",
    "    cols = [c for c in df_xml.columns]\n",
    "    # Selección robusta según nombres habituales de spark-xml\n",
    "    df_books = df_xml.select(\n",
    "        F.col(\"_attr_id\").alias(\"id\") if \"_attr_id\" in cols else F.col(\"id\"),\n",
    "        \"title\",\"author\",\"year\",\n",
    "        F.col(\"price._value\").cast(\"double\").alias(\"price\") if \"price._value\" in cols else F.col(\"price\").cast(\"double\"),\n",
    "        F.col(\"price._attr_currency\").alias(\"currency\") if \"price._attr_currency\" in cols else F.lit(None).alias(\"currency\"),\n",
    "        F.concat_ws(\", \", \"tags.tag\").alias(\"tags\") if \"tags\" in cols else F.lit(None).alias(\"tags\")\n",
    "    )\n",
    "    print(\"Lectura XML con spark-xml OK\")\n",
    "except Exception as e:\n",
    "    print(\"spark-xml no disponible, usando fallback:\", e)\n",
    "\n",
    "if df_books is None:\n",
    "    # Fallback: parseo con Python y creación de DataFrame Spark\n",
    "    import xml.etree.ElementTree as ET\n",
    "    root = ET.fromstring(xml_path.read_text(encoding=\"utf-8\"))\n",
    "    rows = []\n",
    "    for book in root.findall(\"book\"):\n",
    "        book_id = book.attrib.get(\"id\")\n",
    "        title = book.findtext(\"title\")\n",
    "        author = book.findtext(\"author\")\n",
    "        year = int(book.findtext(\"year\"))\n",
    "        price_el = book.find(\"price\")\n",
    "        price = float(price_el.text)\n",
    "        currency = price_el.attrib.get(\"currency\")\n",
    "        tags = [t.text for t in book.findall(\"tags/tag\")]\n",
    "        rows.append((book_id, title, author, year, price, currency, \", \".join(tags)))\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"id\", T.StringType(), False),\n",
    "        T.StructField(\"title\", T.StringType(), True),\n",
    "        T.StructField(\"author\", T.StringType(), True),\n",
    "        T.StructField(\"year\", T.IntegerType(), True),\n",
    "        T.StructField(\"price\", T.DoubleType(), True),\n",
    "        T.StructField(\"currency\", T.StringType(), True),\n",
    "        T.StructField(\"tags\", T.StringType(), True),\n",
    "    ])\n",
    "    df_books = spark.createDataFrame(rows, schema)\n",
    "    print(\"XML leído con fallback y cargado a Spark DataFrame.\")\n",
    "\n",
    "df_books.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51909994",
   "metadata": {},
   "source": [
    "---\n",
    "## 3) Datos **no estructurados** con PySpark\n",
    "\n",
    "**Definición.** Sin esquema predefinido; texto, binarios, multimedia.  \n",
    "**Estrategias.** Para texto: tokenización, stopwords, conteos. Para binarios: lectura como `binaryFile` y UDFs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf9f07",
   "metadata": {},
   "source": [
    "### Ejemplo A: procesamiento básico de **texto** (tokenización + stopwords + regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482fa42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from pyspark.sql import functions as F, types as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "docs = [\n",
    "    (\"politica_seguridad.txt\", '''\n",
    "La política pública de seguridad busca reducir la violencia mediante estrategias \n",
    "de prevención, inteligencia y participación comunitaria. Contacto: seguridad@alcaldia.gov.\n",
    "Reunión programada para 12/03/2025.\n",
    "'''),\n",
    "    (\"informe_tecnico.txt\", '''\n",
    "Este informe técnico evalúa variables ambientales (temperatura y humedad) y su relación\n",
    "con el funcionamiento de equipos. Responsable: carla.romero@universidad.edu.\n",
    "Próxima calibración: 2025-04-15.\n",
    "''')\n",
    "]\n",
    "\n",
    "df_docs = spark.createDataFrame(docs, [\"filename\",\"text\"])\n",
    "\n",
    "# Tokenización con regex (palabras)\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens_raw\", pattern=\"\\W+\")\n",
    "df_tok = tokenizer.transform(df_docs)\n",
    "\n",
    "# Stopwords (lista breve en español, puedes ampliarla según necesidad)\n",
    "stopwords_es = \"\"\"de la el y en que los las un una para es se con por del al lo como mas o\n",
    "mediante su sus este esta eso esa entre sobre a e u y/o\"\"\".split()\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"tokens_raw\", outputCol=\"tokens\", stopWords=stopwords_es, caseSensitive=False)\n",
    "df_clean = remover.transform(df_tok)\n",
    "\n",
    "# Conteo de palabras (excluimos tokens puramente numéricos)\n",
    "df_words = (df_clean\n",
    "            .select(F.explode(\"tokens\").alias(\"word\"))\n",
    "            .filter(~F.col(\"word\").rlike(\"^[0-9]+$\"))\n",
    "            .groupBy(\"word\").count()\n",
    "            .orderBy(F.desc(\"count\"))\n",
    "           )\n",
    "\n",
    "top10_pd = df_words.limit(10).toPandas()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(top10_pd[\"word\"], top10_pd[\"count\"])\n",
    "plt.title(\"Top 10 palabras (PySpark)\")\n",
    "plt.xlabel(\"Palabra\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_words.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eae64e",
   "metadata": {},
   "source": [
    "#### Extracción de patrones (regex): correos y fechas\n",
    "\n",
    "- Opción directa: `regexp_extract` (primer match).  \n",
    "- Opción avanzada: UDF que devuelve **todas** las coincidencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5513f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "import re\n",
    "\n",
    "email_pat = r\"[\\w\\.-]+@[\\w\\.-]+\"\n",
    "date_pat  = r\"(?:\\b\\d{2}/\\d{2}/\\d{4}\\b|\\b\\d{4}-\\d{2}-\\d{2}\\b)\"\n",
    "\n",
    "# Primer match por archivo (puede ser vacío si no hay coincidencia)\n",
    "df_first = df_docs.select(\n",
    "    \"filename\",\n",
    "    F.regexp_extract(\"text\", email_pat, 0).alias(\"email\"),\n",
    "    F.regexp_extract(\"text\", date_pat, 0).alias(\"fecha\")\n",
    ")\n",
    "df_first.show(truncate=False)\n",
    "\n",
    "# Todas las coincidencias usando UDF\n",
    "def find_all(pattern, text):\n",
    "    return re.findall(pattern, text or \"\")\n",
    "\n",
    "find_all_udf = F.udf(find_all, T.ArrayType(T.StringType()))\n",
    "\n",
    "df_all = df_docs.select(\n",
    "    \"filename\",\n",
    "    F.explode_outer(find_all_udf(F.lit(email_pat), F.col(\"text\"))).alias(\"email\"),\n",
    "    F.explode_outer(find_all_udf(F.lit(date_pat),  F.col(\"text\"))).alias(\"fecha\")\n",
    ")\n",
    "df_all.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e37b6f",
   "metadata": {},
   "source": [
    "### Ejemplo: **Imágenes** como binarios + UDF de histograma\n",
    "\n",
    "- Generamos una imagen **en escala de grises** sintética (gradiente 256×256).\n",
    "- La leemos con `binaryFile` y calculamos un **histograma de intensidades** con una UDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear imagen sintética y guardarla\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "img_path = Path(\"/mnt/data/gradient.png\")\n",
    "img_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "arr = np.tile(np.linspace(0, 255, 256, dtype=np.uint8), (256,1))\n",
    "Image.fromarray(arr, mode=\"L\").save(img_path)\n",
    "print(\"Imagen guardada en:\", img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04515cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar binario con Spark y calcular histograma\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_img = spark.read.format(\"binaryFile\").load(str(img_path))\n",
    "\n",
    "def hist256(content: bytes):\n",
    "    im = Image.open(BytesIO(content)).convert(\"L\")\n",
    "    a = np.array(im, dtype=np.uint8)\n",
    "    hist, _ = np.histogram(a, bins=256, range=(0,255))\n",
    "    return hist.tolist()\n",
    "\n",
    "hist_udf = F.udf(hist256, T.ArrayType(T.IntegerType()))\n",
    "\n",
    "hist_df = df_img.select(hist_udf(\"content\").alias(\"hist\"))\n",
    "hist_ex = hist_df.select(F.posexplode(\"hist\").alias(\"intensity\",\"count\"))\n",
    "\n",
    "hist_pd = hist_ex.toPandas()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(hist_pd[\"intensity\"], hist_pd[\"count\"])\n",
    "plt.title(\"Histograma de intensidades (imagen sintética, PySpark)\")\n",
    "plt.xlabel(\"Intensidad (0-255)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "hist_ex.orderBy(\"intensity\").show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc35537",
   "metadata": {},
   "source": [
    "## Cierre de la sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020969b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parar Spark (opcional en notebooks)\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
